\chapter{Discussion}

Postemptive concurrency control strategies naturally flourish in settings with
low contention and aid the programmer especially when that contention doesn't
always have a predetermined structure. As demonstrated, such strategies allow
for levels of parallelism comparable to current methods and adapt well to the
fact that different parallel runs incidentally produce different degrees of
parallelism. But, the lack of explicit contention management serves as perhaps
the most attractive aspect of postemptive strategies. When using preemptive
strategies, the correctness of the program depends on the ability of the
programmer to explicitly manage concurrent reads and writes of shared data.
Historically it has proven a time-consuming and complex task. Postemptive
concurrency control schemes avoid the need for this and allow for reads and
writes to shared data under common and practical use cases. Their downsides
involve potential decreases in the runtime and space efficiency of the program
and the need to supply extra information about the behavior of the program. But,
in practical settings, it remains to be seen whether postemptive methods such as
the one described competes with optimized preemptive methods.

\section{Optimization of Implementation}
There are a number of existing optimizations to CT vectors that are not present
in the current implementation.

\subsection{Memory Allocation of Nodes and Leaves}
One major source of slowdown in the \texttt{ctvector} implementation of the
tests comes from freeing and allocating nodes needed by the vector whenever a
thread  modifies a leaf of a splinter it has not yet modified since detaching.
Many programs involve repeatedly detaching/reattaching as part of an iterative
process, meaning that nodes and leaves will freed and allocated at a similar
pace; furthermore, every node will have the same size as every other node, and
likewise for leaves. A custom allocator could avoid unnecessary allocations by
reusing freed leaves when allocating new ones, which would reduce the overall
time spent allocating and freeing memory.

\begin{comment}
TODO: move to implementation
\subsection{Redundant Storage in Nodes}
The size of the code could be reduced by a fairly
large constant factor of approximately $2$ for 32-bit values (such as
\texttt{int}s or \texttt{float}s on most platforms) and a depth of $5$, which
corresponds to a size of $64^5 \approx 1,000,000,000$ using the default branch
size of 64. This factor diminishes to $1.07$ for storing 1024-bit objects and
increases to $3.4$ for packed boolean values by using a \texttt{std::bitset}
instead of a \texttt{std::array} to store values, although this has not been
implemented. In space-sensitive applications where computations are usually
performed on 32-bit numbers, the space savings above do make a difference.
\end{comment}

\subsection{Tail Optimization}
The CT vector can benefit from a tail optimization where the pointer to
the final leaf is stored in the root. This optimization primarily benefits
inserting at the end of the tree, and since the CT vector was primarily
leveraged for mutations as opposed to inserting and removing elements, this
optimization was not prioritized.

\subsection{Branching Factor}
The CT vector's nodes stores a constant number of branches or values where that
constant is some power of 2, in order to provide an efficient method for lookup
of values through the tree. When the constant is 32 or greater, the runtime of
traveling to a leaf is at most $\log_{32}(n)$ where $n$ is the size of the
vector. It is unfeasible to store more than 256 billion bytes in memory (this
corresponds to a processor that has access to 256 gigabytes of shared memory),
and it is unrealistic to store objects smaller than one byte.
$\log_{32}(256,000,000,000) \approx 7.58$, which means that the depth of the
bit-partitioned trie of nodes and leaves will never exceed 8. With a branching
factor of 64 and a vector with at most 1 billion values, the depth will never
exceed 5. This growth is so slow that it can, for all intents and purposes, be
treated as a constant, which is why a large branching factor is used. But, the
larger the branching factor, the larger the copy when a CT vector sets a value
in a node it has not yet touched, as it must copy entire nodes. Picking a
contextually good branching factor, or even dynamically determining it or using
heterogeneous branching factors for different parts of the trie, could improve
performance.

In the setting of automated parallelism, often the pattern by which splinters
will access and mutate the input vector is known in advance. When this is the
case, it may be possible to choose a branching factor for points in the vector
that corresponds to those access and mutation patterns. It is much more
efficient for splinters to mutate whole leaves, or better yet, whole subtries,
and the shallower the root of the subtrie in the CT vector, the more efficient
the process. Tailoring the branching structure of the trie to confer these ideal
conditions would result in an optimal reattaching and resolution process.
Furthermore, assuming a sparse access and mutation pattern, the trie could even
change its branching factor at runtime to minimize copying and to avoid
unnecessary checking during reattachment and resolution, by keeping the size of
newly-created leaves as small as possible. It remains to be seen whether the extra
logic necessary to manage a bit-partitioned trie with nodes that do not always
have the same branching factor would outweigh the optimizations made possible by
that flexibility, or vice-versa.

\subsection{Computation Ordering to Minimize Contention}
When the range of the indexmap is partitioned among splinters, the domain does
not necessarily partition in the same way. Any values in the domain that lie
outside a given splinter's range are candidates for conflicts. These are the
values checked in the resolution process. By having splinters compute these
values first, the chance that conflicts will occur is minimized.
This optimization has not been implemented, and it remains to be seen if the
extra logic necessary to compute the values in a non-consecutive order can be
implemented without the overhead outweighing the benefits. Furthermore,
performing operations on non-contiguous data can cause cache misses, and so the
unpreditable behavior of hardware might limit the performance of this strategy.
It is likely that there are cases, such as multi-dimensional finite difference
stencils, where the sparsity and pattern of values with potential conflicts are
straightfoward enough to easily prioritize the slated operations.

\subsection{Optimization of operators and indexmaps}
Operators and indexmaps are currently stored in structs with
\texttt{std::function} objects inside them. While this provides an elegant
interface for dealing with functions logically in C++, this standard library
function type does not allow for the same level of optimization as raw C-style
function pointers, or better yet, truly inline operations that map to an
optimized set of assembly instructions. For operations with relatively few (or
even exactly 1) assembly instructions, the overhead of working with
\texttt{std::function} objects adds too much overhead to the execution of the
functions contained within. The inability of the C++ compiler to inline
functions stored as these types exacts the majority of the overhead. A more
efficient interface for managing functions as variables and working with them
symbolically is necessary for this vector to be truly competitive with standard
implementations. Likely this will involve some combination of C-style function
pointers and templated functions that have template parameters that receive
those function pointers.

\subsection{Fully lock-free Reattaching and Resolution}
Currently the CT vector implementation uses two locks: one to lock the map where
trackers are stored when they are emplaced into the map, and another to lock the
dependent vector's bit-partitioned trie when reattaching of reduce-style
partitions and when resolution happens. The first lock is necessary and does not
affect performance, because the emplacement of values into this map and lookups
in the map accounts for very little actual work compared to the amount of work
splinters would do in actual uses cases for parallelisim. The second lock has
two uses. In the first, really a single processor should perfrom this
reattachment, because each processor holding a lock to perform the final reduce
of $P$ or fewer values incurs substantial overhead compared to a simple serial
reduce. In the second, locking prevents the need to resolve again with the same
dependee-dependent pair after resolution takes place if an output vector depends
on multiple input vectors.  The code has not yet been configured to rerun
resolution in the correct sequence to avoid this lock, although as described it
is possible to achieve resolution in a totally lock-free manner.

The only other synchronization primitive used is a
\texttt{std::atomic<int32\_t>} for assigning IDs to CT vectors (and raw
transient vectors, if used).

\section{Improvement of Interface}

\subsection{The Copying Problem}
CT vectors do not have consistent semantics for being copied by value. Currently
when the user calls the copy constructor on a CT vector, it returns a copy of
the underlying bit-partitioned vector with a new ID and an otherwise empty set
of members. Passing a CT vector by value to a method and expecting it to behave
like a genuine copy of the original vector does not work. It will not be able to
serve as a candidate for detaching or reattaching splinters as if it were the
original vector, and additionally will not be able to serve as a base for
resolution of vectors that were produced by using the original vector as input.
This is, of course, the correct behavior, but accidentally calling the copy
constructor in various situations may confuse users.

Developing a better interface and writing a semantically correct move
constructor would solve this problem.

\subsection{The Intermediate Dependencies Problem}
One major concern involves
writing subroutines that perform multiple stepwise computations (where each step
involves a set of splinter detach and reattach calls) from a local function
scope, where the function returns the final output vector. In order to preserve
the asynchronous properties of the CT vector, the intermediate functions must
be resolved outside the local function scope, or else the resolution will be
premature and force synchronization when the function returns. But, there is no
good way of moving those intermediate CT vectors outside of the local function.
Currently, the destructor of the CT vector waits for all detached splinters to
reattach, as otherwise, segfaults will take place while the splinters do try to
reattach. The destructor does not perform forward resolution or wait for itself
to be resolved; doing so would prevent the desired asynchronous properties.
Resolving onto a vector that has been destroyed (by being \texttt{delete}d if
allocated on the heap, or by going out of scope) is currently undefined behavior
in the same way that using any deleted object is undefined behavior. This is why
the interface requires that the vectors being resolved onto are passed manually
as parameters: it forces users to have undeleted and in-scope references to
them in order to perform resolution on them.
