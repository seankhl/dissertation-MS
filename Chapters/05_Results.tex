\chapter{Results}

Results are shown from running benchmarks on  a dual-core Intel IvyBridge
i7-3520M processor with 12GB RAM running Debian Jessie/Stretch using g++ 6.1.1.
The implementation, including the code for the benchmarks from which the
following results derive, can be found at
\url{http://github.com/seanlaguna/contentious}. The code also comes with an
build process that uses autotools. The code depends on the Boost Thread and
System libraries, and on the concurrent producer/consumer queue and LifoSem data
structures in Facebook's open source Folly library. A version of the code
without the dependency on the data structures in Facebook's Folly library
exists, but using these data strutures marginally improved performance
consistency in the following benchmarks. A minimal subset of the Folly library
is provided to avoid the need to install the entire library as a dependency.

\begin{comment}
The second machine has 2 quad-core AMD Opteron 4386 processors with 64GB RAM
running Ubuntu 14.04 LTS using g++ 5.4.0.
\end{comment}

Tests are performed on each machine up to twice the number of physical threads;
the maximum number of threads tested are thus 4. These utilize hyperthreading to
run two simultaneous threads per core, but hyperthreads will not necessarily
provide the same performance as running each thread on its own physical core
\cite{openmp}.

\section{Reduce}
Results shown compare various implementations of a reduce operation of a random
vector of doubles:

\begin{itemize}
 \item ``seq'' is a sequential implementaiton;
 \item ``vec'' is a sequential implementation, but with values always stored in
 vectors (e.g., results of ``reduce'' operations are stored in size-1 vectors).
 This is provided because in order for this method to operate in parallel on
 vectors, the result operations must be itself stored in a ctvector, for
 comparison's sake, tests are performed with the same restriction with a
 std::vector;
 \item ``omp'' is an OpenMP implementation. It uses the maximum number of cores
 available in all cases, so it is also not sensitive to the number of
 threads/cores that are manually selected;
 \item ``avx'' uses a 256-bit AVX instruction to perform the addition of 4 doubles
 simultaneously until only 4 values remain, and then uses more avx instructions
 to sum the resulting values. Thus, the speedup of this implementation can be
 compared directly to other implementations when using 4 concurrent threads;
 \item ``async'' is a C++11-style parallel implementation using the C++ std::async
 feature to launch threads, each of which are given a fraction of the list equal
 to the inverse of the total number of threads.
\end{itemize}

\begin{figure}[!h]
\centering
    \includegraphics[width=0.85\textwidth]{Graphs/boostfunc/reduce_size-v-time.png}
\end{figure}
\begin{figure}[!h]
\centering
    \includegraphics[width=0.85\textwidth]{Graphs/boostfunc/reduce_procs-v-speed-s.png}
\end{figure}
\begin{figure}[!h]
\centering
    \includegraphics[width=0.85\textwidth]{Graphs/boostfunc/reduce_procs-v-speed-m.png}
\end{figure}
\begin{figure}[!h]
\centering
    \includegraphics[width=0.85\textwidth]{Graphs/boostfunc/reduce_procs-v-speed-l.png}
\end{figure}
\begin{figure}[!h]
\centering
    \includegraphics[width=0.85\textwidth]{Graphs/boostfunc/reduce_procs-v-speed-a.png}
\end{figure}

The results indicate that while incurring some overhead, the cont method
scales in a similar way to the async and omp methods. The async benchmark
better represents a low-overhead but manual parallelization scheme: like the
cont benchmark, it does not include strategies to account for hardware details
like the memory layout or cache coherence. Because OpenMP provides a more
opaque, high-level interface for parallelizing a reduce operation, it stands to
reason that the omp benchmark has better performance because of extra tuning
provided by the library itself. Although, other software has further improved
the cache coherence of OpenMP benchmarks, though similar ideas may have been
incorporated into the current OpenMP implementation since then
\cite{cachecoherence}.

\section{Foreach}
Results show scaling for the contentious library performing a foreach operation
on a random vector of doubles. For comparison, the results show the runtime of a
serial implementation as well.

\begin{figure}[!h]
\centering
    \includegraphics[width=0.85\textwidth]{Graphs/boostfunc/foreach_size-v-time.png}
\end{figure}
\begin{figure}[!h]
\centering
    \includegraphics[width=0.85\textwidth]{Graphs/boostfunc/foreach_procs-v-speed-a.png}
\end{figure}

The results follow a similar pattern to those for the reduce benchmarks, except
the baseline single-processor comparison shows that using the \texttt{ctvector}
provides better performance than using \texttt{std::vector} at larger sizes. Because the
\texttt{ctvector} stores data in a non-contiguous manner, the memory allocator
and cache may work more efficiently when trying to allocate or compute upon data
much larger than 4 MB (the size of the L3 cache on the tested machine).

Another test demonstrates the effect on the branching factor of the
bit-partitioned vector used to implement the \texttt{ctvector}. The results show
that a minimum branching factor of $2^{10}$ or 1024 provides approximately the
best performance for the foreach benchmark; choosing the minimum branching
factor allows for minimal copying when performing partial operations on the
vector and otherwise reduces contention between processors. Results did not vary
much depending on the particular benchmark or number of threads used. Other uses
of this data structure, such as by D'orange, indicated that a much lower
branching factor would provide near-optimal performance, such as $2^5$ or $2^6$.
But, the use cases tested often involved random, out-of-order mutations,
insertions, or deletions; our use cases involve almost entirely patterned
mutations that can take better advantage of large, contiguous blocks of values.

\section{Heat}
Results show scaling for the contentious library using a 1st-order scheme to
compute the heat equation framed as a Boundary-Value Problem.

\begin{comment}
With optimization turned on vec behaves a lot like seq. Without
optimization, vec is a lot slower. All these benchmarks use optimization flags,
so the results are almost identical between seq and veche "async"
implementation uses C++ threads with the async
\end{comment}
